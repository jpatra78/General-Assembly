{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Credit yuanyuanshi\n",
    "#https://github.com/yuanyuanshi/Data_Skills/blob/master/data_skills_2.py\n",
    "\n",
    "#Credit:  Massive props to Byron Stuart for helping me getting this code running\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords #conda install nltk\n",
    "\n",
    "#Import Goose here: https://pypi.python.org/pypi/goose-extractor/\n",
    "#Follow instructions.  It is NOT a straight forward pip install\n",
    "from goose import Goose \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "from scrapy.selector import Selector\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#This will only work in jupyter notebook and command line. I tried in ipython and it did not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I get these keywords from the first page search result of data scientist at indeed; they're not whole but already tell a story.\n",
    "program_languages=['bash','r','python','java','c++','ruby','perl','matlab','javascript','scala','php']\n",
    "analysis_software=['excel','tableau','d3.js','sas','spss','d3','saas','pandas','numpy','scipy','sps','spotfire','scikits.learn','splunk','powerpoint','h2o']\n",
    "bigdata_tool=['hadoop','mapreduce','spark','pig','hive','shark','oozie','zookeeper','flume','mahout']\n",
    "databases=['sql','nosql','hbase','cassandra','mongodb','mysql','mssql','postgresql','oracle db','rdbms']\n",
    "education = ['bachelors','bachelor','masters','phd', 'bsc','msc']\n",
    "skills = ['machine learning', 'svm', 'source vector modeling', 'random forest', 'neural network', 'decision trees','mining', 'mining', 'web scraping']\n",
    "overall_dict = program_languages + analysis_software + bigdata_tool + databases + education + skills\n",
    "\n",
    "\n",
    "#extra skills to add = ai, artificial intelligence, tensorflow, nlp, natural language processing,\n",
    "#machine learning, svm, source vector modeling, random forest, neural network, ensemble, decision trees\n",
    "#data mining, mining, webscraping, web scraping, communication skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following two functions are for webpage text processing to extract the skill keywords.\n",
    "def keywords_extract(url):\n",
    "    g = Goose()\n",
    "    article = g.extract(url=url)\n",
    "    text = article.cleaned_text\n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text) #get rid of things that aren't words; 3 for d3 and + for c++\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\")) #filter out stop words in english language\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = list(set(text))\n",
    "    keywords = [str(word) for word in text if word in overall_dict]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for this function, thanks to this blog:https://jessesw.com/Data-Science-Skills/\n",
    "def keywords_f(soup_obj):\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    text = soup_obj.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines()) # break into line\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    text = ''.join(chunk for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:\n",
    "        return\n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)\n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) # Fix spacing issue from merged words\n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = list(set(text)) #only care about if a word appears, don't care about the frequency\n",
    "    keywords = [str(word) for word in text if word in overall_dict] #if a skill keyword is found, return it.\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.seek.com.au/Data-Analyst-jobs?page=1?page=1',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=2',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=3',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=4',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=5',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=6',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=7']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https://www.seek.com.au\"\n",
    "\n",
    "#change the start_url can scrape different cities.\n",
    "#start_url = 'https://www.seek.com.au/Data-Scientist-jobs?page=7'\n",
    "start_url = 'https://www.seek.com.au/Data-Analyst-jobs?page=1'\n",
    "\n",
    "resp = requests.get(start_url)\n",
    "start_soup = BeautifulSoup(resp.content,'html.parser')\n",
    "\n",
    "domain = 'https://www.seek.com.au'\n",
    "\n",
    "#Find number urls (jobs on first page)\n",
    "urls=[]\n",
    "for entry in start_soup.find_all('a',{'class':'_1OFaluu'},href=True):\n",
    "    urls.append(domain+entry['href'])\n",
    "\n",
    "#When we search for data science jobs we see there are 7 pages of jobs\n",
    "#Will eventuallyuse a function to cycle through each page\n",
    "\n",
    "\n",
    "#Create list of the pages\n",
    "job_page=[]\n",
    "for u in range(1,8):\n",
    "    job_page.append(start_url+'?'+'page='+str(u))\n",
    "job_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#From Eric Nessi\n",
    "\n",
    "def job_func(job_title='Data+Analyst', city = None, state = None, page_number=None):\n",
    "# job_title ='Data+Analyst' \n",
    "# city = None\n",
    "# state = None\n",
    "# page_number=None\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split()\n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        # Join all of our strings together so that indeed will search correctly        start_url_list = ['https://www.seek.com.au/jobs/in-All-', final_city, '-', state, '?keywords=%22', job_title, '%22']\n",
    "    else:\n",
    "        start_url_list = ['https://www.seek.com.au/jobs?keywords=%22', job_title, '%22']\n",
    "    \n",
    "    start_url = ''.join(start_url_list)    \n",
    "    #change the start_url can scrape different cities.\n",
    "\n",
    "    domain = 'https://www.seek.com.au'\n",
    "\n",
    "    #Find number urls (jobs on first page)\n",
    "    page_number = 20\n",
    "    #page_number + 1\n",
    "    \n",
    "    job_page = []\n",
    "    job_page.append(start_url)\n",
    "    for u in range(2,page_number):\n",
    "        job_page.append(start_url+'&'+'page='+str(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_analyst_urls = \n",
    "job_func(job_title='Data+Analyst', city = None, state = None, page_number = 20)\n",
    "#len(data_analyst_urls)\n",
    "#data_analyst_urls[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-ff68bd6244c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_sci_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Data+Scientist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sci_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "data_sci_urls = job_func(job_title='Data+Scientist', city = None, state = None, page_number = 8)\n",
    "len(data_sci_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytics_con_urls = job_func(job_title='Analytics+Consultant', city = None, state = None, page_number = 5)\n",
    "len(analytics_con_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bus_intel_urls = job_func(job_title='business+intelligence', city = None, state = None, page_number = 38)\n",
    "len(bus_intel_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_sci_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3dd9d46a8b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Merge all urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0murls_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_analyst_urls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_sci_urls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0manalytics_con_urls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbus_intel_urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls_merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Looks like we have over 1000 jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_sci_urls' is not defined"
     ]
    }
   ],
   "source": [
    "#Merge all urls\n",
    "urls_merge = data_analyst_urls + data_sci_urls + analytics_con_urls + bus_intel_urls\n",
    "len(urls_merge)\n",
    "\n",
    "#Looks like we have over 1000 jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1124, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.seek.com.au/job/33431855?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.seek.com.au/job/33431139?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.seek.com.au/job/33431716?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.seek.com.au/job/33430694?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.seek.com.au/job/33428799?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls\n",
       "0  https://www.seek.com.au/job/33431855?type=stan...\n",
       "1  https://www.seek.com.au/job/33431139?type=stan...\n",
       "2  https://www.seek.com.au/job/33431716?type=stan...\n",
       "3  https://www.seek.com.au/job/33430694?type=stan...\n",
       "4  https://www.seek.com.au/job/33428799?type=stan..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for duplicate urls\n",
    "urls_df = pd.DataFrame({'urls':urls_merge})\n",
    "print urls_df.shape\n",
    "urls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1124, 1)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(urls_df).drop_duplicates()\n",
    "urls_df.shape\n",
    "\n",
    "#no duplicate urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls=urls_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=urls_merge\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls_df.to_csv('./urls_df.csv')\n",
    "#a.to_csv(\"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.seek.com.au/Data-Analyst-jobs?page=1?page=1',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=2',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=3',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=4',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=5',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=6',\n",
       " 'https://www.seek.com.au/Data-Analyst-jobs?page=1?page=7']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "base_url = \"https://www.seek.com.au\"\n",
    "\n",
    "#change the start_url can scrape different cities.\n",
    "#start_url = 'https://www.seek.com.au/Data-Scientist-jobs?page=7'\n",
    "start_url = 'https://www.seek.com.au/Data-Analyst-jobs?page=1'\n",
    "\n",
    "resp = requests.get(start_url)\n",
    "start_soup = BeautifulSoup(resp.content,'html.parser')\n",
    "\n",
    "domain = 'https://www.seek.com.au'\n",
    "\n",
    "#Find number urls (jobs on first page)\n",
    "urls=[]\n",
    "for entry in start_soup.find_all('a',{'class':'_1OFaluu'},href=True):\n",
    "    urls.append(domain+entry['href'])\n",
    "\n",
    "#When we search for data science jobs we see there are 7 pages of jobs\n",
    "#Will eventuallyuse a function to cycle through each page\n",
    "\n",
    "\n",
    "#Create list of the pages\n",
    "job_page=[]\n",
    "for u in range(1,8):\n",
    "    job_page.append(start_url+'?'+'page='+str(u))\n",
    "job_page\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls',\n",
       " 'https://www.seek.com.au/job/33431855?type=standout&tier=no_tier&pos=1&whereid=3000&userqueryid=e8af3ba9d327b21d8e0161b035a84567-7472012&ref=beta',\n",
       " 'https://www.seek.com.au/job/33431139?type=standout&tier=no_tier&pos=2&whereid=3000&userqueryid=e8af3ba9d327b21d8e0161b035a84567-7472012&ref=beta']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_csv = '/Users/gopetro/documents/dsi/materials/project4/urls1.csv'\n",
    "import csv\n",
    "with open(urls_csv, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    urls = list(reader)\n",
    "\n",
    "#Flatten list\n",
    "from itertools import chain\n",
    "\n",
    "urls = list(chain.from_iterable(urls))\n",
    "    \n",
    "#urls=urls[1:]\n",
    "urls[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open Chrome driver\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/gopetro/documents/dsi/code/chromedriver\")\n",
    "get_info = True\n",
    "\n",
    "#Firefox\n",
    "#webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "#get_info = True\n",
    "#driver=webdriver.Firefox(executable_path=\"/Users/gopetro/documents/dsi/code/geckodriver\")\n",
    "\n",
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split urls\n",
    "#urls1 = urls[0:140]\n",
    "# urls2 = urls[162:280]\n",
    "# urls3 = urls[281:420]\n",
    "# urls3a = urls[394:420]\n",
    "# urls4 = urls[421:560]\n",
    "# urls4a = urls[445:560]\n",
    "# urls5 = urls[561:700]\n",
    "# urls5a= urls[631:700]\n",
    "# urls5b= urls[667:700]\n",
    "# urls6 = urls[701:840]\n",
    "# urls7 = urls[841:980]\n",
    "# urls7a = urls[909:981]\n",
    "# urls8 = urls[981:1124]\n",
    "# urls8a=urls[1007:1124]\n",
    "\n",
    "#urls2a = urls[258:280]\n",
    "urls3b = urls[345:420]\n",
    "urls3c = urls[406:420]\n",
    "urls4a = urls[490:560]\n",
    "urls4b = urls[513:560]\n",
    "urls4c = urls[522:560]\n",
    "urls5a= urls[650:700]\n",
    "#urls6 = urls[581:700]\n",
    "urls6a = urls[750:850]\n",
    "\n",
    "#urls5b= urls[667:700]\n",
    "#urls6 = urls[701:840]\n",
    "#urls7 = urls[841:980]\n",
    "#urls7a = urls[909:981]\n",
    "#urls8 = urls[981:1124]\n",
    "#urls8a=urls[1007:1124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As we run search let's create list for each.  We will merge each list at the end to make dataframe\n",
    "Date=[]\n",
    "Title=[]\n",
    "Location=[]\n",
    "Salary =[]\n",
    "Employment_Type =[]\n",
    "Industry =[]\n",
    "skills = []\n",
    "Title =[]\n",
    "Education =[]\n",
    "job_keywords=[]\n",
    "\n",
    "len(urls6a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting 1 job keywords...\n",
      "['excel'] 4\n",
      "extracting 2 job keywords...\n",
      "[] 4\n",
      "extracting 3 job keywords...\n",
      "['tableau', 'python', 'r', 'sas', 'spark', 'hadoop', 'scala', 'spss'] 4\n",
      "extracting 4 job keywords...\n",
      "['tableau', 'python', 'sas', 'java', 'splunk', 'sql'] 4\n",
      "extracting 5 job keywords...\n",
      "['hadoop', 'sql'] 4\n",
      "extracting 6 job keywords...\n",
      "[] 4\n",
      "extracting 7 job keywords...\n",
      "['tableau', 'sql'] 4\n",
      "extracting 8 job keywords...\n",
      "['java', 'sql'] 4\n",
      "extracting 9 job keywords...\n",
      "['excel', 'tableau', 'sql'] 4\n",
      "extracting 10 job keywords...\n",
      "['excel'] 4\n",
      "extracting 11 job keywords...\n",
      "['tableau', 'r', 'sas'] 4\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\u2013' in position 35: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-52072c434d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#df.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mdf6a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./urls_df6a.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gopetro/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gopetro/anaconda2/lib/python2.7/site-packages/pandas/formats/format.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1473\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gopetro/anaconda2/lib/python2.7/site-packages/pandas/formats/format.pyc\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gopetro/anaconda2/lib/python2.7/site-packages/pandas/formats/format.pyc\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m   1600\u001b[0m                                         quoting=self.quoting)\n\u001b[1;32m   1601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1602\u001b[0;31m         \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;31m# from collections import namedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/lib.pyx\u001b[0m in \u001b[0;36mpandas.lib.write_csv_rows (pandas/lib.c:21184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\u2013' in position 35: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "#Base code for doing a scan of one page\n",
    "#THis one works\n",
    "\n",
    "#for i in range(1,4):\n",
    "#for i in range(5,len(urls))\n",
    "#for i in range(len(urls)):\n",
    "for i in range(1,len(urls6a)):    \n",
    "    get_info = True\n",
    "    try:\n",
    "        #requests.get(urls[i].text)\n",
    "        driver.get(urls6a[i])\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,5000)/1000.0\n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        print 'extracting %d job keywords...' % i\n",
    "        single_job = keywords_f(soup)\n",
    "        print single_job,len(soup)\n",
    "        #print driver.current_url\n",
    "        #job_keywords.append([driver.urls[i],single_job])\n",
    "        skills.append(single_job)\n",
    "        \n",
    "        #job_keywords.append([driver.current_url,single_job])\n",
    "        #skills.append(single_job)\n",
    "        \n",
    "        #Byron's addition\n",
    "        #print 'Date posted'\n",
    "        #print soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True)\n",
    "        Date.append(soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True))\n",
    "        #print 'Job title:', soup.find('h1').get_text(\" \", strip=True)\n",
    "        Title.append(soup.find('h1').get_text(\" \", strip=True))\n",
    "            #print(job_page.find('h1', {'class': 'jobtitle'}).get_text(\" \", strip=True))\n",
    "        \n",
    "        #print soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True)\n",
    "        Location.append(soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True))\n",
    "\n",
    "        if soup.find('div', {'itemprop': 'baseSalary'}) == None:\n",
    "        #    print 'Salary:', 'None stated'\n",
    "            Salary.append(soup.find('div', {'itemprop': 'baseSalary'}))\n",
    "        else:\n",
    "        #    print 'Salary:', soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True)\n",
    "            Salary.append(soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True))\n",
    "        \n",
    "        \n",
    "        #print 'Employment Type:', soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True)\n",
    "        Employment_Type.append(soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True))\n",
    "            \n",
    "        \n",
    "        \n",
    "        #print 'Industry:', soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True)\n",
    "        Industry.append(soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True))\n",
    "\t\t#\tmod_job_details = job_page.find('div', {'class': 'mod-job-details'})\n",
    "        #    mod_job_details = mod_job_details.find_all('div')\n",
    "        #    div_list = list(mod_job_details)\n",
    "        #print 'Industry:', div_list[len(div_list)-1].get_text(\" \", strip=True)\n",
    "        df6a=pd.DataFrame({'Date_Posted':Date,\n",
    "                             'Location':Location,\n",
    "                             'Title':Title,\n",
    "                             'Type':Employment_Type,\n",
    "                             'Salary':Salary,\n",
    "                             'Industry':Industry,\n",
    "                             'Skills':skills})\n",
    "        \n",
    "        #df.head()\n",
    "        df6a.to_csv('./urls_df6a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.shape\n",
    "df2.to_csv('./urls_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.seek.com.au/job/33431139?type=standout&tier=no_tier&pos=2&whereid=3000&userqueryid=e8af3ba9d327b21d8e0161b035a84567-7472012&ref=beta']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-2889822ed01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mget_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#driver.get(urls[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Base code for doing a scan of one page\n",
    "\n",
    "#for i in range(1,4):\n",
    "#for i in range(5,len(urls))\n",
    "#for i in range(len(urls)):\n",
    "for i in range(1,len(urls4a)):    \n",
    "    get_info = True\n",
    "    try:\n",
    "        requests.get(urls[i].text)\n",
    "        #driver.get(urls[i])\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        print 'extracting %d job keywords...' % i\n",
    "        single_job = keywords_f(soup)\n",
    "        print single_job,len(soup)\n",
    "        #print driver.current_url\n",
    "        #job_keywords.append([driver.urls[i],single_job])\n",
    "        skills.append(single_job)\n",
    "        \n",
    "        #job_keywords.append([driver.current_url,single_job])\n",
    "        #skills.append(single_job)\n",
    "        \n",
    "        #Byron's addition\n",
    "        #print 'Date posted'\n",
    "        #print soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True)\n",
    "        Date.append(soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True))\n",
    "        #print 'Job title:', soup.find('h1').get_text(\" \", strip=True)\n",
    "        Title.append(soup.find('h1').get_text(\" \", strip=True))\n",
    "            #print(job_page.find('h1', {'class': 'jobtitle'}).get_text(\" \", strip=True))\n",
    "        \n",
    "        #print soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True)\n",
    "        Location.append(soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True))\n",
    "\n",
    "        if soup.find('div', {'itemprop': 'baseSalary'}) == None:\n",
    "        #    print 'Salary:', 'None stated'\n",
    "            Salary.append(soup.find('div', {'itemprop': 'baseSalary'}))\n",
    "        else:\n",
    "        #    print 'Salary:', soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True)\n",
    "            Salary.append(soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True))\n",
    "        \n",
    "        \n",
    "        #print 'Employment Type:', soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True)\n",
    "        Employment_Type.append(soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True))\n",
    "            \n",
    "        \n",
    "        \n",
    "        #print 'Industry:', soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True)\n",
    "        Industry.append(soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True))\n",
    "\t\t#\tmod_job_details = job_page.find('div', {'class': 'mod-job-details'})\n",
    "        #    mod_job_details = mod_job_details.find_all('div')\n",
    "        #    div_list = list(mod_job_details)\n",
    "        #print 'Industry:', div_list[len(div_list)-1].get_text(\" \", strip=True)\n",
    "        df4a=pd.DataFrame({'Date_Posted':Date,\n",
    "                             'Location':Location,\n",
    "                             'Title':Title,\n",
    "                             'Type':Employment_Type,\n",
    "                             'Salary':Salary,\n",
    "                             'Industry':Industry,\n",
    "                             'Skills':skills})\n",
    "        \n",
    "        #df.head()\n",
    "        df4a.to_csv('./urls_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#yens code\n",
    "## initialize DataFrame\n",
    "data = pd.DataFrame(columns=['title','salary','location','advertiser', 'industry','description'])\n",
    "\n",
    "def get_job_info(job_url):\n",
    "    html = requests.get(job_url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    # Get the row\n",
    "    try:\n",
    "        title = soup.find(\"h1\",\"jobtitle\").string\n",
    "    except Exception:\n",
    "        title = 'NaN'\n",
    "    # to take into account cases where salary is not listed, I want to write\n",
    "    #salary = NaN if soup.find(\"div\", {'itemprop':\"baseSalary\"}).renderContents() not valid\n",
    "    try:\n",
    "        salary = soup.find(\"div\", {'itemprop':\"baseSalary\"}).renderContents()\n",
    "    except Exception:\n",
    "        salary = 'NaN'\n",
    "    \n",
    "    try:\n",
    "        location = soup.find(\"span\", {'data-automation':'apply_text_location_area'}).renderContents()\n",
    "    except Exception:\n",
    "        location = 'NaN'\n",
    "    try:\n",
    "        advertiser = soup.find(\"span\", {'class':'state-message'}).renderContents()\n",
    "    except Exception:\n",
    "        advertiser = 'NaN'\n",
    "    try:\n",
    "        industry = soup.find(\"span\", {'itemprop':'industry'}).renderContents()\n",
    "    except Exception:\n",
    "        industry = 'NaN'\n",
    "    try:\n",
    "        description = soup.find(\"div\", {'class':'templatetext'}).renderContents()\n",
    "    except Exception:\n",
    "        description = 'NaN'   \n",
    "    # add to data dataframe\n",
    "    data.loc[len(data.title)]=[title,salary,location,advertiser,industry,description]\n",
    "    # add a counter so I can see what's happenning\n",
    "    if len(data.title) >0:\n",
    "        print(len(data.title))\n",
    "    data.to_csv('./data_df3.csv')\n",
    "    \n",
    "#        single_job = keywords_f(soup)\n",
    "#        print single_job,len(soup)\n",
    "#        #print driver.current_url\n",
    "#        #job_keywords.append([driver.urls[i],single_job])\n",
    "#        skills.append(single_job)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,len(urls3)):\n",
    "    get_job_info(urls3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 7)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nframes = [df1, df2, df3, df4, df5, df6, df7]\\n\\nds_result = pd.concat(frames)\\nds_result.to_csv('ds_result.csv')\\nprint ds_result.shape\\nds_result.head(30)\\n\""
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "frames = [df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "ds_result = pd.concat(frames)\n",
    "ds_result.to_csv('ds_result.csv')\n",
    "print ds_result.shape\n",
    "ds_result.head(30)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This function gets the job listing page, gets the url link for each job and scans the information\n",
    "def page(job_page):\n",
    "    #job_page is first page to scan\n",
    "    \n",
    "    start_url = job_page #Set the page with jobs\n",
    "    resp = requests.get(start_url)\n",
    "    start_soup = BeautifulSoup(resp.content,'html.parser') #Use beautifulsoup to scrape the page\n",
    "\n",
    "    domain = 'https://www.seek.com.au'\n",
    "\n",
    "    #Find number urls (jobs on first page)\n",
    "    urls=[]\n",
    "    for entry in start_soup.find_all('a',{'class':'_1OFaluu'},href=True):\n",
    "        urls.append(domain+entry['href'])\n",
    "    print len(urls)\n",
    "  \n",
    "    # open the driver\n",
    "    driver = webdriver.Chrome(executable_path=\"/Users/gopetro/documents/dsi/code/chromedriver\")\n",
    "    get_info = True\n",
    "\n",
    "    # set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "    driver.set_page_load_timeout(15)\n",
    "    \n",
    "    #As we run search need to reset each list to maintain same dimensions\n",
    "    #We will merge each list at the end to make dataframe\n",
    "    Date=[]\n",
    "    Title=[]\n",
    "    Location=[]\n",
    "    Salary =[]\n",
    "    Employment_Type =[]\n",
    "    Industry =[]\n",
    "    skills = []\n",
    "    Title =[]\n",
    "    Education =[]\n",
    "    job_keywords=[]\n",
    "    \n",
    "    for i in range(0,2):\n",
    "    #for i in range(len(urls)):\n",
    "        get_info = True\n",
    "        try:\n",
    "            driver.get(urls[i])\n",
    "        except TimeoutException:\n",
    "            get_info = False\n",
    "            continue\n",
    "        j = random.randint(1000,2200)/1000.0\n",
    "        time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "        \n",
    "        if get_info:\n",
    "            \n",
    "            soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            print 'extracting %d job keywords...' % i\n",
    "            single_job = keywords_f(soup)\n",
    "            print single_job,len(soup)\n",
    "            #print driver.current_url\n",
    "            #job_keywords.append([driver.urls[i],single_job])\n",
    "            skills.append(single_job)\n",
    "            \n",
    "            #Byron's addition\n",
    "            #print 'Date posted'\n",
    "            #print soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True)\n",
    "            Date.append(soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True))\n",
    "            #print 'Job title:', soup.find('h1').get_text(\" \", strip=True)\n",
    "            Title.append(soup.find('h1').get_text(\" \", strip=True))\n",
    "            #print(job_page.find('h1', {'class': 'jobtitle'}).get_text(\" \", strip=True))\n",
    "        \n",
    "            #print soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True)\n",
    "            Location.append(soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True))\n",
    "\n",
    "            if soup.find('div', {'itemprop': 'baseSalary'}) == None:\n",
    "                #print 'Salary:', 'None stated'\n",
    "                Salary.append(soup.find('div', {'itemprop': 'baseSalary'}))\n",
    "            else:\n",
    "                #print 'Salary:', soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True)\n",
    "                Salary.append(soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True))\n",
    "        \n",
    "        \n",
    "            #print 'Employment Type:', soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True)\n",
    "            Employment_Type.append(soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True))\n",
    "            \n",
    "        \n",
    "        \n",
    "            #print 'Industry:', soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True)\n",
    "            Industry.append(soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True))\n",
    "            #mod_job_details = job_page.find('div', {'class': 'mod-job-details'})\n",
    "            #mod_job_details = mod_job_details.find_all('div')\n",
    "            #div_list = list(mod_job_details)\n",
    "            #print 'Industry:', div_list[len(div_list)-1].get_text(\" \", strip=True)\n",
    "#     df=pd.DataFrame({'Date_Posted':Date,\n",
    "#                      'Location':Location,\n",
    "#                      'Title':Title,\n",
    "#                      'Type':Employment_Type, \n",
    "#                      'Salary':Salary,\n",
    "#                      'Industry':Industry,\n",
    "#                      'Skills':skills})\n",
    "#             # Write save it to a csv file\n",
    "#             #df.to_csv('./page.csv', index = False)\n",
    "\n",
    "    return df\n",
    "    \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "extracting 0 job keywords...\n",
      "['python', 'r', 'spark', 'hadoop', 'scala', 'sql'] 4\n",
      "extracting 1 job keywords...\n",
      "['python', 'r'] 4\n"
     ]
    }
   ],
   "source": [
    "page('https://www.seek.com.au/Data-Scientist-jobs?page=1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5f4b6f1ca735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Location' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting 1 job keywords...\n",
      "['matlab', 'hadoop', 'python', 'mining', 'r', 'spark', 'sql'] 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Posted</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Title</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 May 2017</td>\n",
       "      <td>Information &amp; Communication Technology</td>\n",
       "      <td>Location: ACT</td>\n",
       "      <td>None</td>\n",
       "      <td>[matlab, hadoop, python, mining, r, spark, sql]</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Full Time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date_Posted                                Industry       Location Salary  \\\n",
       "0  3 May 2017  Information & Communication Technology  Location: ACT   None   \n",
       "\n",
       "                                            Skills           Title       Type  \n",
       "0  [matlab, hadoop, python, mining, r, spark, sql]  Data Scientist  Full Time  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('https://www.seek.com.au/Data-Scientist-jobs?page=2')\n",
    "#Searching 'Data Scientist'\n",
    "#page1 = page(job_page[0])\n",
    "#page('https://www.seek.com.au/Data-Scientist-jobs?page=2')\n",
    "#page3 = page(job_page[2])\n",
    "#page4 = page(job_page[3])\n",
    "#page5 = page(job_page[4])\n",
    "#page6 = page(job_page[5])\n",
    "#page7 = page(job_page[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting 1 page of job searching results...\n",
      "extracting 2 page of job searching results...\n"
     ]
    }
   ],
   "source": [
    "#Do not run!\n",
    "#This will run for each page.\n",
    "\n",
    "for k in range(1,3):\n",
    "#for k in range(1,num_pages+1):    \n",
    "#this 5 pages reopen the browser is to prevent connection refused error.\n",
    "    if k%5==0:\n",
    "        #driver.quit()\n",
    "        driver = webdriver.Chrome(executable_path=\"/Users/gopetro/documents/dsi/code/chromedriver\")\n",
    "        #driver=webdriver.Firefox()\n",
    "        driver.set_page_load_timeout(15)\n",
    "    current_url = start_url + str(k)\n",
    "    print 'extracting %d page of job searching results...' % k\n",
    "    resp = requests.get(current_url)\n",
    "    current_soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    #current_soup = BeautifulSoup(resp.content)\n",
    "    #current_urls = current_soup.findAll('a',{'rel':'nofollow','target':'_blank'})\n",
    "    #current_urls = [link['href'] for link in current_urls]\n",
    "    \n",
    "    current_urls =[]\n",
    "    for entry in start_soup.find_all('a',{'class':'_1OFaluu'},href=True):\n",
    "        current_urls.append(domain+entry['href'])\n",
    "    \n",
    "    for i in range(1,4):\n",
    "    #for i in range(5,len(urls))\n",
    "    #for i in range(len(urls)):\n",
    "        get_info = True\n",
    "        try:\n",
    "            #driver.get(base_url+urls[i])\n",
    "            #driver.get(current_urls[i])\n",
    "            driver.get(urls[i])\n",
    "        except TimeoutException:\n",
    "            get_info = False\n",
    "        continue\n",
    "        j = random.randint(1000,2200)/1000.0\n",
    "        time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "        if get_info:\n",
    "            soup=BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            print 'extracting %d job keywords...' % i\n",
    "            single_job = keywords_f(soup)\n",
    "            print single_job,len(soup)\n",
    "            print driver.current_url\n",
    "            job_keywords.append([driver.current_url,single_job])\n",
    "            skills.append(single_job)\n",
    "            #Byron's addition\n",
    "            print 'Date posted'\n",
    "            print soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True)\n",
    "            Date.append(soup.find('strong', {'itemprop': 'datePosted'}).get_text(\" \", strip=True))\n",
    "            print 'Job title:', soup.find('h1').get_text(\" \", strip=True)\n",
    "            Title.append(soup.find('h1').get_text(\" \", strip=True))\n",
    "            #print(job_page.find('h1', {'class': 'jobtitle'}).get_text(\" \", strip=True))\n",
    "            \n",
    "            print soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True)\n",
    "            Location.append(soup.find('li', {'itemprop': 'jobLocation'}).get_text(\" \", strip=True))\n",
    "            if soup.find('div', {'itemprop': 'baseSalary'}) == None:\n",
    "                print 'Salary:', 'None stated'\n",
    "                Salary.append(soup.find('div', {'itemprop': 'baseSalary'}))\n",
    "            else:\n",
    "                print 'Salary:', soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True)\n",
    "                Salary.append(soup.find('div', {'itemprop': 'baseSalary'}).get_text(\" \", strip=True))\n",
    "            \n",
    "            print 'Employment Type:', soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True)\n",
    "            Employment_Type.append(soup.find('div', {'itemprop': 'employmentType'}).get_text(\" \", strip=True))\n",
    "            \n",
    "            print 'Industry:', soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True)\n",
    "            Industry.append(soup.find('span', {'itemprop': 'industry'}).get_text(\" \", strip=True))\n",
    "# use driver.quit() not driver.close() can get rid of the openning too many files error.\n",
    "#driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ed6f3c7bd2d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#skills_dict_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mskills_dict_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_keywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mskills_dict_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'job_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "#skills_dict_1 = [w[1] for w in job_keywords]\n",
    "#skills_dict_1\n",
    "\n",
    "skills_dict_2 = [w[1] for w in job_keywords]\n",
    "skills_dict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Combine into dictionarysdict={}\n",
    "for words in skills_dict:\n",
    "    for word in words:\n",
    "        if not word in sdict:\n",
    "            sdict[word]=1\n",
    "        else:\n",
    "            sdict[word]+=1\n",
    "Result = pd.DataFrame()\n",
    "Result['Skill'] = sdict.keys()\n",
    "Result['Count'] = sdict.values()\n",
    "Result['Ranking'] = Result['Count']/float(len(job_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert to DataFrame\n",
    "Result = pd.DataFrame()\n",
    "Result['Skill'] = sdict.keys()\n",
    "Result['Count'] = sdict.values()\n",
    "Result['Ranking'] = Result['Count']/float(len(job_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skill</th>\n",
       "      <th>Count</th>\n",
       "      <th>Ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>java</td>\n",
       "      <td>33</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rdbms</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scala</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>python</td>\n",
       "      <td>56</td>\n",
       "      <td>9.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saas</td>\n",
       "      <td>12</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mysql</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>excel</td>\n",
       "      <td>15</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hadoop</td>\n",
       "      <td>7</td>\n",
       "      <td>1.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c++</td>\n",
       "      <td>15</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tableau</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>perl</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>r</td>\n",
       "      <td>52</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sql</td>\n",
       "      <td>34</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sas</td>\n",
       "      <td>13</td>\n",
       "      <td>2.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spark</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>php</td>\n",
       "      <td>12</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>powerpoint</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Skill  Count   Ranking\n",
       "0         java     33  5.500000\n",
       "1        rdbms      3  0.500000\n",
       "2        scala      4  0.666667\n",
       "3       python     56  9.333333\n",
       "4         saas     12  2.000000\n",
       "5        mysql      2  0.333333\n",
       "6        excel     15  2.500000\n",
       "7       hadoop      7  1.166667\n",
       "8          c++     15  2.500000\n",
       "9      tableau      2  0.333333\n",
       "10        perl      1  0.166667\n",
       "11           r     52  8.666667\n",
       "12         sql     34  5.666667\n",
       "13         sas     13  2.166667\n",
       "14       spark      4  0.666667\n",
       "15         php     12  2.000000\n",
       "16  powerpoint      2  0.333333"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
